<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Resources for Further Learning | Bias within the AI Environment</title>
  <meta name="description" content="Chapter 6 Resources for Further Learning | Bias within the AI Environment" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Resources for Further Learning | Bias within the AI Environment" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Resources for Further Learning | Bias within the AI Environment" />
  
  
  

<meta name="author" content="Drew, Grace, Vivi, Xinyan, Yibo" />


<meta name="date" content="2023-08-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mitigating-biases-in-ai-systems.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> url: your book url like https://bookdown.org/yihui/bookdown</a></li>
<li class="chapter" data-level="2" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i><b>2</b> About Us</a>
<ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#grace"><i class="fa fa-check"></i>Grace</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#vivi"><i class="fa fa-check"></i>Vivi</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#yibo"><i class="fa fa-check"></i>Yibo</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#xinyan"><i class="fa fa-check"></i>Xinyan</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#drew"><i class="fa fa-check"></i>Drew</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#as-a-team"><i class="fa fa-check"></i>As a Team</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html"><i class="fa fa-check"></i><b>3</b> Overview of AI Biases</a>
<ul>
<li class="chapter" data-level="" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#systemic-bias"><i class="fa fa-check"></i>Systemic Bias</a></li>
<li class="chapter" data-level="" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#automation-bias"><i class="fa fa-check"></i>Automation Bias</a></li>
<li class="chapter" data-level="" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#overfitting-and-underfitting"><i class="fa fa-check"></i>Overfitting and Underfitting</a></li>
<li class="chapter" data-level="" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#reporting-bias"><i class="fa fa-check"></i>Reporting bias</a></li>
<li class="chapter" data-level="" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#overgeneralization-bias"><i class="fa fa-check"></i>Overgeneralization bias</a></li>
<li class="chapter" data-level="" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#group-attribution-bias"><i class="fa fa-check"></i>Group Attribution bias</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html"><i class="fa fa-check"></i><b>4</b> Overview of AI and LLM</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#artificial-intelligence"><i class="fa fa-check"></i><b>4.1</b> Artificial Intelligence</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#narrow-ai"><i class="fa fa-check"></i><b>4.1.1</b> Narrow AI</a></li>
<li class="chapter" data-level="4.1.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#general-ai"><i class="fa fa-check"></i><b>4.1.2</b> General AI</a></li>
<li class="chapter" data-level="4.1.3" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works"><i class="fa fa-check"></i><b>4.1.3</b> How It Works</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#large-language-models"><i class="fa fa-check"></i><b>4.2</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works-1"><i class="fa fa-check"></i><b>4.2.1</b> How It Works</a></li>
<li class="chapter" data-level="4.2.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-is-trained"><i class="fa fa-check"></i><b>4.2.2</b> How It Is Trained</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#biases-in-hiring-lending-criminal-justice"><i class="fa fa-check"></i><b>4.3</b> <strong>Biases in hiring, lending, criminal justice:</strong></a></li>
<li class="chapter" data-level="4.4" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#inherent-biases---the-complex-trade-offs-in-designing-fair-machine-learning-models."><i class="fa fa-check"></i><b>4.4</b> <strong>Inherent Biases - the complex trade-offs in designing fair machine learning models.</strong></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html"><i class="fa fa-check"></i><b>5</b> Mitigating Biases in AI systems</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#company-policies-and-guidelines"><i class="fa fa-check"></i><b>5.1</b> Company Policies and Guidelines</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#ibm"><i class="fa fa-check"></i><b>5.1.1</b> IBM</a></li>
<li class="chapter" data-level="5.1.2" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#google-responsible-ai-practices"><i class="fa fa-check"></i><b>5.1.2</b> Google Responsible AI Practices</a></li>
<li class="chapter" data-level="5.1.3" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#openai"><i class="fa fa-check"></i><b>5.1.3</b> OpenAI</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#what-do-we-do-about-bias-as-a-user-or-business-leader"><i class="fa fa-check"></i><b>5.2</b> What Do We Do About Bias as a User or Business Leader?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html"><i class="fa fa-check"></i><b>6</b> Resources for Further Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#more-research-into-ai-bias"><i class="fa fa-check"></i><b>6.1</b> More Research into AI Bias</a></li>
<li class="chapter" data-level="6.2" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#transparency-in-ai-bias"><i class="fa fa-check"></i><b>6.2</b> Transparency in AI Bias:</a></li>
<li class="chapter" data-level="6.3" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#fairness-in-ai-bias"><i class="fa fa-check"></i><b>6.3</b> Fairness in AI Bias:</a></li>
<li class="chapter" data-level="6.4" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#importance-of-ongoing-studies"><i class="fa fa-check"></i><b>6.4</b> Importance of Ongoing Studies</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bias within the AI Environment</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="resources-for-further-learning" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Resources for Further Learning<a href="resources-for-further-learning.html#resources-for-further-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="more-research-into-ai-bias" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> More Research into AI Bias<a href="resources-for-further-learning.html#more-research-into-ai-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The integration of technology into societal problem-solving has led to both remarkable advancements and complex challenges. While technical aspects may appear exciting, ethical concerns, fairness considerations, and broader societal implications require continuous exploration and refinement. The emergence of AI Bias as a crucial topic prompts us to delve into its multifaceted dimensions, where transparency and fairness always stand as two of the most pivotal research directions.</p>
<p><img src="AI.PNG" /></p>
</div>
<div id="transparency-in-ai-bias" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Transparency in AI Bias:<a href="resources-for-further-learning.html#transparency-in-ai-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><img src="Transparency.png" />
In the paper “Towards A Rigorous Science of Interpretable Machine Learning” by Dr. Lipton, the significance of building a comprehensive framework for interpretable machine learning is highlighted. As machine learning models become increasingly intricate, their lack of interpretability poses challenges to understanding and trusting their decisions. The paper advocates a systematic approach involving evaluation metrics, domain knowledge integration, and human feedback to enhance interpretability.
However, the concept of transparency has extended beyond technical realms in many cases; it usually also encompasses data privacy concerns in everyday situations, as evidenced by the examination of AI’s role in biomedical research and consent processes. In the article “AI, big data, and the future of consent”, Dr. Kearn mentioned that patients have almost no autonomy in the re-purposed data problem. Likewise, Ai Bias also arises when users are not given alternative choices in cases where they do not wish to consent. In fact, users who currently do not wish to comply with the terms and conditions lack the power to renegotiate.</p>
</div>
<div id="fairness-in-ai-bias" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Fairness in AI Bias:<a href="resources-for-further-learning.html#fairness-in-ai-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><img src="Faireness.png" />
Addressing fairness in AI goes beyond pure algorithmic solutions. Dr. Barocas proposes a holistic approach in “Fairness and Abstraction in Sociotechnical Systems” emphasizing the amalgamation of algorithmic design and systemic societal factors to achieve meaningful fairness. Similarly, Dr. Hardt’s work on “Equality of Opportunity in Supervised Learning” introduces fairness metrics that balance error rates among different subgroups while maximizing classifier accuracy. Such R&amp;D cases are all going to underscore the importance of considering broader societal contexts and interdisciplinary collaboration in ensuring fairness.</p>
</div>
<div id="importance-of-ongoing-studies" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Importance of Ongoing Studies<a href="resources-for-further-learning.html#importance-of-ongoing-studies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>AI Bias as a nascent topic is an ongoing revolution, the significance will keep intensifying as AI becomes further integrated into society. Whether in the workplace, healthcare, or marginalized group treatment, the awareness of biases and issues must be met with a comprehensive response that transcends individual disciplines. Therefore, the interdisciplinary nature of this challenge mandates the involvement of more and more far studies from diverse perspectives, as highlighted by the various research directions discussed.
Overall, starting from the realization of AI Bias existence, continuous learning, research, and collaboration are essential to navigate the complexities of AI Bias. And is going to further guide the AI evolution towards a genuinely beneficial technology for the whole society.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mitigating-biases-in-ai-systems.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/06-share.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
