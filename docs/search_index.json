<<<<<<< HEAD
[["overview-of-ai-biases.html", "Chapter 3 Overview of AI Biases Systemic Bias Automation Bias Overfitting and Underfitting Reporting bias Overgeneralization bias Group Attribution bias", " Chapter 3 Overview of AI Biases What really is AI bias? In short, it is when specific components within a dataset are allowed to be overrepresented or weighted too highly. This, in turn, forces other components to become underrepresented and underweighted. Consequently, accuracy can get affected, results can be misshapen, which allows prejudices to occur. At this point in time, most companies and organizations are beginning to adopt artificial intelligence as part of their overall business intelligence strategy. As far as the how and the why, it’s difficult to parse an agreed upon set of bias types (if the magnitude of the set even closely resembles that of the set of cognitive biases, one can expect over 100). This is mostly because different people use different terms for the same thing. On top of that, bias can find its way into every part of the AI process, namely from the data itself, from the algorithm, or from the people meant to interpret the result. That being said, here are a collection of the most prevalent types of bias: Systemic Bias Systemic bias looks as far outward as possible and examines the ways in which entire social structures and groups are overlooked in favor of others. As the name indicates, this kind of discrimination comes as a result of well established systematic processes that dominate our lives. Most importantly, these processes are often designed in such a way that makes them invisible to most people. Machine learning algorithms that produce AI models are inherently confined to build and interpret from the data that they are given. They primarily function on pattern recognition, which is unfortunately the main heuristic that stereotyping utilizes. Thus when feeding them data that is representative of the systems that we all belong to, they inherit the same invisible discriminations. The data that these algorithms operate on is called training data. The reality is that this data is not solely restricted to what a single set of real world measurements can offer us. There are data experts that collect and synthesize information from many different sources into a unified set that can add new categories of measurement or target specific perspectives that might have gone unnoticed. Not to mention the possibility of creating entirely synthetic data from artificial generators, which similarly allows engineers to filter out the unwanted behavior that might come from model interpretation of the data. A good example of this systematic behavior within AI came out several years ago, when a group of researchers at Carnegie Mellon and the International Computer Science Institute investigated how Google’s Ad Settings chose to target specific ads towards certain people. The results were unfavorable to say the least. Instances of discrimination occurred across the board from Google’s algorithm. A notable case was when the researchers changed the gender on the Ad Settings to female. What they noticed was that ads related to higher paying jobs were reduced in the female setting as opposed to the male. In this case, the model was perpetuating the discriminatory behavior of standard advertisers when they unknowingly reinforce gender roles. Automation Bias On the spectrum of data error versus human error, automation bias stands firmly in the latter category. In simple terms, it is when humans overly rely on automation. Given AI’s totemic position within the vanguard of automated processes, one can clearly see how this form of bias can be abused. Of all of the discourse surrounding AI today, conversations about the future of reliance on AI is perhaps the most zeitgeist piercing. The real cause for concern around this form of bias is the expectation that AI produces infallible results. If the training data on the model is in any way meant to be representative of the real world, then it suffers the same tendencies for inaccuracy that we do. If that data happens to be the Internet itself, then it suffers even greater chances to spread misinformation. We are often told to not make decisions off of a single source. Paradoxically, a result from a single AI model — whose goal it is to scour an innumerable amount of sources — unifies itself into a single source. In Hugo Mercier and Dan Sperber’s “Why do humans reason? Arguments for an argumentative theory”, they suggest that the evolutionary skill of reasoning served an persuasive function rather than one built for the purpose of knowledge acquisition. Humans inherently (and often unconsciously) start from a conclusion first and search for the appropriate evidence to support it. The implication here is that, on our own, we are actually pretty bad at coming to the most correct conclusion. However, together within an argumentative setting of other people, we are more effectively able to pick out the right components of each argument into a single realized conclusion. The reason to bring this up is that the inherent processes of machine learning does not create its conclusions from argumentation because who would it be arguing against? Even the more advanced machine reasoning, which attempts to replicate human “common sense” through logical processes, is totally at odds with this theory. This misalignment can help to explain why GPS can often lead you down the wrong path in an attempt to find the quickest path. ChatGPT presents the most visible example of automation bias today. Educators at every level have been struggling to get kids to not use the AI to do work for them whether that be assignments or assessments. Kids will receive answers that, regardless of validity, provide none of the reasoning process that got it there. This inherent problem of not knowing or understanding how a machine learning algorithm got to a certain conclusion is called black-box and it is only increasingly becoming a problem as more and more of the general public adopts AI usage. Overfitting and Underfitting Imagine you are trying to research a certain topic with the goal in mind that you can be trusted to give reliable information regarding that topic. In order to do so, you try to find as many sources as possible. The idea is that you would reduce the amount of key missing information. However, now you have so much data that there’s noise and contradictions. The only way to make a conclusion that fits all of the data is an overly convoluted one. This is one of the fundamental issues happening with machine learning models right now. Underfitting is in the former instance when a model isn’t able to capture the trend in testing data despite performing well in the training data. Typically this occurs from insufficient amounts of training data which results in the model producing simplified rules for a more complicated problem. This is where AI can often make biased decisions. Using overly simple patterns, it can generalize these to entire categories of data. The upside is that its variance in accuracy depending on the testing data remains fairly low. This is not to say it’s somehow accurate, just that it’s equally inaccurate across many different testing environments. Conversely, overfitting is when there is more than sufficient data but the noise that comes from it allows the model to produce far too complicated results that forces the data to fit. Applying different testing environments creates high variance in inaccuracy even though the model is trying to consider every possible manner of bias. This is the rub, high bias but for low variance, or low bias at the cost of high variance. This model provides a great visualization for the pros and cons of underfitting and overfitting and how typically it is beneficial to find a balance between the two. One of the most often used ways to detect overfitting is called k-fold cross-validation. The idea is to randomly sort the data into k subsections, saving one section to be the testing set and the others will become the training data. Iterate this k times, taking the cumulative sum of the errors to see how well the model performs. As it must be apparent at this point, neither underfitting nor overfitting is strictly better than the other. More importantly, in tandem they reveal an unfortunate reality of any AI model: it is impossible to create a system with zero bias and zero variance in accuracy. The best we can do is find the optimized minimum of the two. Reporting bias Moving outside of machine error, we arrive at the problem of how we choose what data to train an algorithm with. Reporting bias is most often seen when an algorithm is measuring event frequency. The bias happens when the training data does not accurately reflect the frequency of said event in the real world. A simplified example might be a sentiment analysis model for determining whether a book review is positive or negative. The issue is that most websites that have such reviews (think Goodreads) have an unusually high amount of extreme reviews. This is because most people will only write a review if they really loved or they really hated this. This means that the algorithm will be unequipped to interpret any subtleties. Thus, when trying to synthesize an appropriate dataset for training, we run into a problem: all the most available sources are biased against the intended result. Overgeneralization bias When we look at the output that an AI model generates for us we often take it as gospel. There’s an expectation that what we can see from one dataset can be extrapolated to other datasets accessing the same information but this isn’t usually the case. Machine learning models fundamentally work from probabilities, and imperfect probability is the enemy of repeatable certainty. Group Attribution bias When data engineers are in the process of creating an algorithm, they might favor certain factors that they can attribute to themselves over factors that exist to outside groups. This creates the dichotomy of the in-group versus out-group. Where this type of bias has been prevalent is in the use of AI for hiring and recruitment practices. For example, say the engineers added some extra weight to the particular undergraduate institutions they attended. There is no reason to believe that individuals who attended those institutions exhibit the required skills for the position any more so than those who went to other universities. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
<<<<<<< HEAD
[["mitigating-biases-in-ai-systems.html", "Chapter 6 Mitigating Biases in AI systems 6.1 Company Policies and Guidelines 6.2 What Do We Do About Bias as a User or Business Leader?", " Chapter 6 Mitigating Biases in AI systems 6.1 Company Policies and Guidelines 6.1.1 IBM The AI Fairness 360 (AIF360) toolkit provided by IBM aims to promote a deeper understanding of fairness metrics and mitigation techniques, to enable an open common platform for fairness researchers and industry practitioners to share and benchmark their algorithms, and to help facilitate the transition of fairness research algorithms to use in an industrial setting. AIF360 uses techniques from 8 published papers on the border algorithm fairness community. It includes over 71 bias detection metrics, 9 bias mitigation algorithms, and a unique extensible metric explanation facility to help consumers of the system to understand the meaning of bias detection results. Definitions Used in AIF360 Fairness measures → Provide several fairness metrics, including difference of means, disparate impact, and odds ratio. FairML → Provides an auditing tool for predictive models by quantifying the relative effects of various inputs on a model’s predictions. FairTest → Checks for associations between predicted labels and protected attributes. It allows users to identify regions of the input space where an algorithm might incur unusually high errors. Aequitas → Auditing toolkit for data scientists and policymakers. It has a Python library and an associated website where data can be uploaded for bias analysis. Themis-ML → Repository that provides a few fairness metrics, such as mean difference, and some bias mitigation algorithms, such as relabeling, additive counterfactually fair estimator, and reject option classification. Paradigm and Architecture Architecture A fairness pipeline is a process that ensures that the AI system or algorithm being developed and deployed treats all individuals or groups fairly and avoids perpetuating biases or discriminatory outcomes. A fairness pipeline shows data loading into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from the classifier. Dataset Classes Training data is used to learn classifiers. Testing data is used to make predictions and compare metrics. Besides these standard aspects of the machine learning pipeline, fairness applications also require associating protected attributes with each instance or record in the data. Bias Mitigation AIF360 attempts to improve the fairness metrics by modifying the training data, the learning algorithm, or the predictions - also known as pre-processing, in-processing, and post-processing, respectively. Maintaining Code Quality Establishing and maintaining high-quality code is crucial for an evolving open-source system. The AIF360 Github repository is directly integrated with Travis CI, a continuous testing and integration framework, which invoked “pytest” to run unit tests. Unit test cases ensure that classes and functions defined in the different libraries are functionally correct and do not break the fairness detection and mitigation pipeline flow. The toolkit is enforced in both front-end and back-end services. 6.1.2 Google Responsible AI Practices Fairness AI system’s influence goes beyond mere recommendations, encompassing critical functions such as decision-making in business. These automated systems possess the potential to enhance fairness and inclusivity on a more significant level compared to human-centric judgments. It is vital to recognize that unfairness within AI systems can have far-reaching consequences, inherently showing the importance of fairness. However, achieving fairness in AI is not without its challenges. Machine learning models are trained on real-world data, which inherently carries biases. Constructing universally fair systems is complicated due to the myriad of diverse situations they encounter. Additionally, fairness needs a standardized definition and involves many considerations that must be addressed. Efforts to ensure fairness in AI necessitate ongoing research and dedication. This involves fostering diversity within the workforce and knowledge base, meticulously scrutinizing training data for biases, training models to rectify these biases, evaluating models for performance disparities, and subjecting final systems to rigorous fairness testing. Furthermore, AI can identify human biases and barriers, which can contribute to fostering positive societal changes. The pursuit of fairness in AI remains a dual-edged prospect—an opportunity for progress and a complex challenge to conquer. Companies like Google are committed to advancing fairness, offering tools and resources to the broader community to contribute to this ongoing endeavor. Interpretability Automated predictions and decision-making are pivotal in enhancing various parts of life. In this context, interpretability assumes a crucial role, serving as an anchor for questioning, comprehending, and instilling trust in AI systems. By providing interpretability, AI systems contribute to aligning domain expertise, societal principles, and the development of models. Interestingly, the challenges surrounding explaining decisions are shared by both AI and human decision-making processes. Unlike humans, AI systems can furnish in-depth insights into the rationale behind their predictions, an attribute that sets them apart. However, comprehending intricate AI models, including neural networks, remains a formidable task, even for seasoned experts. Evaluating AI systems introduces unique challenges that distinguish them from conventional software. While traditional software follows explicit if-then rules, AI operates through intricate parameter pathways. The tenets of responsible AI design allow for tracing values back to training data, facilitating the detection of bugs and anomalies. The comprehension of AI hinges upon its training data, the underlying process, and the resultant model. Collective endeavors within the technology community are actively contributing to enhancing the understanding, control, and debugging of AI systems. In this context, Google emerges as a prominent participant, engaging in ongoing research and development endeavors aimed at refining the interpretability of AI. Privacy Machine learning models derive insights from training and input data, raising privacy concerns when handling sensitive information. Achieving a balance between the advantages of using sensitive data and the associated privacy implications is crucial. This encompasses legal, regulatory, social, and individual factors. Implementing safeguards is essential to preserve individual privacy while employing ML models, necessitating transparent practices and user data control. Techniques can mitigate the likelihood of models exposing underlying data. Google is at the forefront of developing strategies to safeguard privacy within AI systems, contributing to an evolving field of research with substantial growth potential. Their willingness to share insights further enhances the collective understanding of privacy protection in the AI domain. Safety Ensuring AI systems align with intentions and remain resilient against attacks is integral to safety and security. This is particularly important in safety-critical applications, necessitating robust safety measures before deployment. Addressing safety challenges becomes intricate in scenarios characterized by unpredictability and generative AI. This complexity is magnified in intricate problems and generative AI. Striking a balance between proactive safety precautions and accommodating creative adaptability presents a formidable task. The evolution of AI introduces novel attack vectors, prompting an ongoing pursuit of practical solutions. When drawing from accumulated insights, it is advisable to prioritize safety during pre-deployment planning for critical applications, anticipate diverse scenarios—particularly in generative AI settings—maintain equilibrium between safety protocols and creative latitude, and remain flexible in countering emerging attack strategies through adaptive countermeasures. Recommended Practices Research in machine learning safety encompasses various threats, including data poisoning, data recovery, model theft, and adversarial examples. Google plays an active role in investigating these domains, some of which intersect with AI and privacy concerns. A specific emphasis lies on adversarial learning, where one network crafts deceptive instances and another discerns fraudulent behavior. Though ongoing, the establishment of dependable defenses against adversarial examples remains in progress. Developers are advised to assess potential attacks and their ramifications, steering clear of vulnerable system designs. Adversarial testing systematically scrutinizes ML models using malicious input to bolster comprehension and mitigation efforts. This process aids in recognizing patterns of failure and guides enhancements through fine-tuning and protective measures. 6.1.3 OpenAI How ChatGPT’s Behavior is Shaped ChatGPT’s model is a massive neural network, unlike ordinary software. The behavior of the model is learned from diverse data, not explicitly programmed. The process is similar to training a dog, unlike traditional programming. There are two phases of training the model, the first of which is the “pre-training” phase. This phase teaches the model to predict the following word from Internet text. Second is the “fine-tuning” phase which fine-tunes the system’s behavior. The model is also constantly improving its alignment with human values. Here is a more in-depth explanation of these training phases and processes: Pre-Training: Predicting the following words from an Internet dataset Learn grammar, facts, and reasoning abilities Absorb biases from the data Fine-Tuning: Narrow dataset with human reviewers Reviewers follow guidelines Not all possible inputs covered Categories in guidelines for model responses Models generalize from reviewer feedback Reviewers and OpenAI policies in system development: OpenAI maintains a close relationship with its reviewers and adheres to its established policies. This includes providing clear instructions on the types of outputs expected from the model. OpenAI offers high-level guidance, particularly concerning handling controversial topics. Collaboration with reviewers is a continuous effort, involving weekly meetings to exchange feedback and seek clarifications. The development process follows an iterative approach, enabling consistent model refinement for improvement over time. Role of Reviewers and OpenAI’s Policies in System Development Reviewer Guidance: Specific output cases (e.g., no illegal content) High-level guidance (e.g., avoid controversial stances) Ongoing collaboration, not one-time Learning from reviewer expertise Feedback Loop and Fine-Tuning Strong feedback loop with reviewers Weekly meetings to address questions Clarify guidance as needed Iterative process for continuous improvement Addressing Bias Addressing Biases and Transparency: OpenAI acknowledges the concerns about AI biases and is committed to addressing them. OpenAI sees bias as bugs, as opposed to features of the model. In order to tackle the issue, they strive to be transparent in their intentions and progress in building the model. In addition, there is no favoring of any political groups by reviewers. Sharing Insights and Accountability: OpenAI shares its guidelines for user insight and to stay accountable for ushering in sound policies. Continuous Improvement: To improve the model, OpenAI is constantly enhancing its guidelines for clarity, making more explicit instructions for reviewers, and addressing bias, controversial figures, and themes. Additionally, they are transparent in the demographic information of their reviewers, continuously research understandable and controllable fine-tuning, and build on external advances like rule-based rewards and Constitutional AI. The Building Blocks of Future Systems The overarching mission centers on ensuring widespread access, benefits, and impact of AI and AGI. To achieve this, three fundamental building blocks are in place. The first focuses on enhancing the default behavior of AI, aiming for usefulness and respectfulness right from the start. This involves comprehensive research and engineering to mitigate biases, rectify over-refusals and under-refusals, and leverage user feedback to drive improvements. The second block revolves around defining AI’s values within appropriate boundaries, facilitating customizable AI systems that align with societal limits. This entails refining ChatGPT for user-driven behavior customization while balancing customization potential and guarding against misuse risks. The goal is to set well-defined boundaries for AI behavior to prevent the concentration of power. The third block emphasizes public involvement in shaping defaults and boundaries to prevent undue centralization of influence. Through collective decision-making, public participation is sought in determining system defaults and behavior boundaries, with external input garnered through red teaming. This extends to soliciting public insights on AI in education and piloting initiatives for public contributions on behavior, disclosure, and policies. Furthermore, exploring third-party safety and policy audits underscores a commitment to transparency and accountability. 6.2 What Do We Do About Bias as a User or Business Leader? Human Biases and AI Human bias can be found in algorithms because of the way that AI systems learn to make decisions. AI learns from training data, which can include biased human decisions or reflect historical or social inequities that inform the AI’s model. Even when sensitive variables such as gender or race are removed, human biases can still be found in the training data. Furthermore, bias can come from flawed data sampling, in which certain groups need to be better represented. Historical Bias and Complex Algorithms Throughout history, various examples illustrate the insidious nature of bias in AI. Instances, where facial recognition systems struggled to identify individuals with darker skin tones accurately serve as a stark reminder of algorithmic bias. While AI has the potential to mitigate biases by automating decision-making, the complexity of algorithms can also amplify existing biases when trained on skewed datasets. Understanding this balance is pivotal in harnessing AI’s potential for societal benefit. Addressing Bias in AI In order to address the bias found in AI, we must have a basic understanding, and a measure of fairness must be conducted. Researchers have a technical way of defining fairness, which can include requiring models to have equal predictive value across groups or that models have equal false positive and false negative rates across groups. Even with these requirements, there are many different definitions of fairness, and they cannot all be satisfied simultaneously. While fairness definitions and metrics evolve in AI, researchers have advanced techniques to align AI systems with these criteria. These techniques involve pre-processing data, post-processing decisions, and integrating fairness into training. “Counterfactual fairness” stands out, ensuring consistent decisions if sensitive attributes like race or gender are changed. Silvia Chiappa’s path-specific counterfactual fairness approach accounts for complex cases where fairness varies along different influences. However, challenges like determining release fairness and appropriate automated decision contexts demand interdisciplinary insights from ethicists, social scientists, and humanities experts. Imperatives for Action It is suggested that business leaders stay up to date on the research of AI since it is a fast-paced field. In addition, leaders should establish responsible processes that can mitigate bias for their employees; examples of this include a portfolio of technical tools or operational practices that audit AI responses. Leaders should also continue to engage in fact-based conversations around the evidence of human biases in AI. By running algorithms monitored by humans, it is possible to compare results using “explainability techniques,” which understand what led the model to reach its decision. Investment in the AI field’s diversity can also help mitigate human bias. A more diverse AI community would be better equipped to anticipate, review, and spot bias and engage the affected communities. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======

[["mitigating-biases-in-ai-systems.html", "Chapter 6 Mitigating Biases in AI systems 6.1 Company Policies and Guidelines 6.2 What Do We Do About Bias as a User or Business Leader?", " Chapter 6 Mitigating Biases in AI systems 6.1 Company Policies and Guidelines 6.1.1 IBM The AI Fairness 360 (AIF360) toolkit provided by IBM aims to promote a deeper understanding of fairness metrics and mitigation techniques, to enable an open common platform for fairness researchers and industry practitioners to share and benchmark their algorithms, and to help facilitate the transition of fairness research algorithms to use in an industrial setting. The AI Fairness 360 (AIF360) toolkit provided by IBM aims to promot a deeper understanding of fairness metrics and mitigation techniques, to enable an open common platform for fairness researchers and industry practitioners to share and benchmark their algorithms, and to help facilitate the transition of fairness research algorithms to use in an industrial setting. AIF360 uses techniques from 8 published papers on the border algorithm fairness community. It includes over 71 bias detection metrics, 9 bias mitigation algorithms, and a unique extensible metric explanation facility to help consumers of the system to understand the meaning of bias detection results Definitions Used in AIF360 Fairness measures → provide several fairness metrics including difference of means, disparate impact, and odds ratio FairML → Provides an auditing tool for predictive models by quantifying the relative effects of various inputs on a model’s predictions FairTest → Checks for associations between predicted labels and protected attributes. Provides user with ability to identify regions of the input space where ana glorithm might incur unusually high errors Aequitas → Auditing toolkit for data scientists and policy makers. It has a Python library as well as an associated website where data can be uploaded for bias analysis Themis-ML → Repository that provides a few fairness metrics, such as mean difference, and some bias mitigation algorithms, such as relabeling, additive counterfactually fair estimator, and reject option classification. Paradigm and Architecture Paradigm and Architecture Fairness pipeline that shows data loading into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from the classifier. Dataset Classes Training data is used to learn classifiers Testing data is used to make predictions and compare metrics Besides these standard aspects of the machine learning pipeline, fairness applications also require associating protected attributes with each instance or record in the data Bias Mitigation AIF360 attempts to improve the fairness metrics by modifying the training data, the learning algorithm, or the predictions - also known as pre-processing, in-processing, and post-processing respectively Maintaining Code Quality Establishing and maintaining high-quality code is crucial for an evolving open-source system. The AIF360 Github repository is directly integrated with Travis CI, a continuous testing and integration framework, which invoked “pytest” to run unit tests. Unit test cases ensure that classes and functions defined in the different libraries are functionally correct and do not break the fairness detection and mitigation pipeline flow. The toolkit is enforced in both front-end and back-end services 6.1.2 Google Responsible AI Practices Fairness AI systems influence goes beyond mere recommendations, encompassing critical functions such as decision-making in business. These automated systems possess the potential to enhance fairness and inclusivity on a larger level compared to human-centric judgments. It is vital to recognize that unfairness within AI systems can have far-reaching consequences, inherently showing the importance of fairness. However, achieving fairness in AI is not without its challenges. Machine learning models are trained on real-world data, which inherently carries biases. Constructing universally fair systems is complicated due to the myriad of diverse situations they encounter. Additionally, fairness lacks a standardized definition and involves a multitude of considerations that must be addressed. Efforts to ensure fairness in AI necessitate ongoing research and dedication. This involves fostering diversity within the workforce and knowledge base, meticulous scrutiny of training data for biases, training models to rectify these biases, evaluating models for any performance disparities, and subjecting final systems to rigorous fairness testing. Furthermore, AI has the capacity to identify human biases and barriers, which can contribute to fostering positive societal changes. The pursuit of fairness in AI remains a dual-edged prospect—an opportunity for progress and a complex challenge to conquer. Companies like Google are committed to advancing fairness, offering tools and resources to the broader community to contribute to this ongoing endeavor. Interpretability Automated predictions and decision-making play a pivotal role in enhancing various parts of life. In this context, the concept of interpretability assumes a crucial role, serving as an anchor for questioning, comprehending, and instilling trust in AI systems. By providing interpretability, AI systems contribute to the alignment of domain expertise, societal principles, and the development of models. Interestingly, the challenges surrounding explaining decisions are shared by both AI and human decision-making processes. Unlike humans, AI systems have the capacity to furnish in-depth insights into the rationale behind their predictions, an attribute that sets them apart. However, comprehending intricate AI models, including neural networks, remains a formidable task, even for seasoned experts. The evaluation of AI systems introduces unique challenges that distinguish them from conventional software. While traditional software follows explicit if-then rules, AI operates through intricate parameter pathways. The tenets of responsible AI design allow for the tracing of values back to training data, facilitating the detection of bugs and anomalies. The comprehension of AI hinges upon its training data, the underlying process, and the resultant model. Collective endeavors within the technology community are actively contributing to the enhancement of understanding, control, and debugging of AI systems. In this context, Google emerges as a prominent participant, engaging in ongoing research and development endeavors aimed at refining the interpretability of AI. Privacy Machine learning models derive insights from training and input data, raising privacy concerns when handling sensitive information. Achieving a balance between the advantages of using sensitive data and the associated privacy implications is crucial. This encompasses legal, regulatory, social, and individual factors. Implementing safeguards is essential to preserve individual privacy while employing ML models, which necessitates transparent practices and user data control. Techniques can mitigate the likelihood of models exposing underlying data. Google is at the forefront of developing strategies to safeguard privacy within AI systems, contributing to an evolving field of research with substantial growth potential. Their willingness to share insights further enhances the collective understanding of privacy protection in the AI domain. Safety Ensuring AI systems align with intentions and remain resilient against attacks is integral to safety and security. This holds particular importance in safety-critical applications, necessitating robust safety measures prior to deployment. Addressing safety challenges becomes intricate in scenarios characterized by unpredictability and generative AI. This complexity is magnified in intricate problems and generative AI. Striking a balance between proactive safety precautions and accommodating creative adaptability presents a formidable task. The evolution of AI introduces novel attack vectors, prompting an ongoing pursuit of effective solutions. Drawing from accumulated insights, it’s advisable to prioritize safety during pre-deployment planning for critical applications, anticipate diverse scenarios—particularly in generative AI settings—maintain equilibrium between safety protocols and creative latitude, and remain flexible in countering emerging attack strategies through adaptive countermeasures. Recommended Practices Research in machine learning safety encompasses various threats including data poisoning, data recovery, model theft, and adversarial examples. Google plays an active role in investigating these domains, some of which intersect with AI and privacy concerns. A specific emphasis lies on adversarial learning, where one network crafts deceptive instances and another discerns fraudulent behavior. Though ongoing, the establishment of dependable defenses against adversarial examples remains in progress. Developers are advised to assess potential attacks and their ramifications, steering clear of vulnerable system designs. Adversarial testing systematically scrutinizes ML models using malicious input to bolster comprehension and mitigation efforts. This process aids in recognizing patterns of failure and guides enhancements through fine-tuning and protective measures. 6.1.3 OpenAI How ChatGPT’s Behavior is Shaped ChatGPT’s model is a massive neural network, unlike ordinary software. The behavior of the model is learned from diverse data, not explicitly programmed. The process is similar to training a dog, as oposed to traditional programming. There are two phases of training the model, the first of which is the “pre-training” phase. This phase consists of teaching the model to predict the next words from Internet text. Second is the “fine-tuning” phase which fine-tunes the system’s behavior. The model is also constantly improving its alighment with human values. Here is a more in-depth explanation of these training phases and processes: Pre-Training: Predicting the next words from the Internet dataset Learn grammar, facts, reasoning abilities Absorb biases from the data Fine-Tuning: Narrow dataset with human reviewers Reviewers follow guidelines Not all possible inputs covered Categories in guidelines for model responses Models generalize from reviewer feedback Reviewers and OpenAI policies in system development: OpenAI maintains a close relationship with its reviewers and adheres to its established policies. This includes providing clear instructions on the types of outputs expected from the model. OpenAI offers high-level guidance, particularly with regards to handling controversial topics. Collaboration with reviewers is a continuous effort, involving weekly meetings to exchange feedback and seek clarifications. The development process follows an iterative approach, enabling consistent refinement of the model for improvement over time. Role of Reviewers and OpenAI’s Policies in System Development Reviewer Guidance: Specific output cases (e.g., no illegal content) High-level guidance (e.g., avoid controversial stances) Ongoing collaboration, not one-time Learning from reviewer expertise Feedback Loop and Fine-Tuning Strong feedback loop with reviewers Weekly meetings to address questions Clarify guidance as needed Iterative process for continuous improvement Addressing Bias Addressing Biases and Transparency: OpenAI acknoledges the concerns about AI biases, and is commited to addressing them. OpenAI sees bias as bugs, as opposed to features of the model. In order to tackle the issue, they strive to be transparent in their intentions and progress in building the model. In addition, there is no favoring of any political groups by reviewers. Sharing Insights and Accountability: OpenAI shares their guidelines for user insight, and to stay accountable for uphering sound policies. Continuous Improvement: In order to improve the model, OpenAI is constantly enhancing their guidelines for clarity, making clearer instructions for reviewers, addressing bias, controversial figures, and themes. Additionaly, they are transparent in the demographic information of their reviewers, continuously research understandable and controllable fine-tuning, and build on external advances like rule-based rewards and Constitutional AI The Building Blocks of Future Systems The overarching mission centers on ensuring widespread access, benefits, and impact of AI and AGI. To achieve this, three fundamental building blocks are in place. The first focuses on enhancing the default behavior of AI, aiming for usefulness and respectfulness right from the start. This involves comprehensive research and engineering to mitigate biases, rectify over-refusals and under-refusals, and leverage user feedback to drive improvements. The second block revolves around defining AI’s values within appropriate boundaries, facilitating customizable AI systems that align with societal limits. This entails refining ChatGPT for user-driven behavior customization while striking a balance between customization potential and guarding against misuse risks. The goal is to set well-defined boundaries for AI behavior to prevent the concentration of power. The third block emphasizes public involvement in shaping defaults and boundaries to prevent undue centralization of influence. Through collective decision-making, public participation is sought in determining system defaults and behavior boundaries, with external input garnered through red teaming. This extends to soliciting public insights on AI in education and piloting initiatives for public contributions on behavior, disclosure, and policies. Furthermore, the exploration of third-party safety and policy audits underscores a commitment to transparency and accountability. 6.2 What Do We Do About Bias as a User or Business Leader? Human Biases and AI Human bias can be found in algorithms because of the way that AI systems learn to make decisions. AI learns from training data, which can include biased human decisions or reflect hisotrical or social inequities, which then inform the AI’s model. Even when sensitive variables such as gender or race are removed, the human biases can still be found in the training data. Furthermore, bias can come from flawed data sampling, in which certain groups are not well-represented. Historical Bias and Complex Algorithms Throughout history, various examples illustrate the insidious nature of bias in AI. Instances where facial recognition systems struggled to accurately identify individuals with darker skin tones serve as a stark reminder of algorithmic bias. While AI has the potential to mitigate biases by automating decision-making, the complexity of algorithms can also amplify existing biases when trained on skewed datasets. Understanding this balance is pivotal in harnessing AI’s potential for societal benefit. Addressing Bias in AI In order to address the bias found in AI, we must have a basic understanding and a measure of fairness must be conducted. Researchers have a technical way of defining fairness, which can include requiring models to have equal predictive value across groups or requiring that models have equal false positive and false negative rates across groups. Even with these requirements, there are many different definitions of fairness and they cannot all be satisfied simultaneously. While fairness definitions and metrics evolve in AI, researchers have advanced techniques to align AI systems with these criteria. These techniques involve pre-processing data, post-processing decisions, and integrating fairness into training. “Counterfactual fairness” stands out, ensuring consistent decisions if sensitive attributes like race or gender were changed. Silvia Chiappa’s path-specific counterfactual fairness approach accounts for complex cases where fairness varies along different influences. However, challenges like determining release fairness and appropriate automated decision contexts demand interdisciplinary insights from ethicists, social scientists, and humanities experts. Imperatives for Action It is suggested that business leaders stay up to-date on the research of AI since it is a fast-paced field. In addition, leaders should be establishing responsible processes that can mitigate bias for their employees; examples of this include a portfolio of technical tools or operational practices which audit AI responses. Leaders should also continue to engage in the fact-based conversations around the evidence of human biases in AI. By running algorithms that are monitored by humans, it is possible to compare results using “explainability techniques” which understands what led the model to reach the decision it did. Investment into the diversity of the AI field can also help to mitigate human bias. A more diverse AI community would be better equipped to anticipate, review, and spot bias and engage the affected communities. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

>>>>>>> b813626a41873c161fd114af38cf9efcfe4ab1f4
>>>>>>> 4317c3a2828f100bdc41feda1b518635a91a2ecc
