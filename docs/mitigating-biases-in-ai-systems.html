<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Mitigating Biases in AI Systems | Bias within the AI Environment</title>
  <meta name="description" content="Chapter 6 Mitigating Biases in AI Systems | Bias within the AI Environment" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Mitigating Biases in AI Systems | Bias within the AI Environment" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Mitigating Biases in AI Systems | Bias within the AI Environment" />
  
  
  

<meta name="author" content="Drew, Grace, Vivi, Xinyan, Yibo" />


<meta name="date" content="2023-08-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="case-studies.html"/>
<link rel="next" href="resources-for-further-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i><b>2</b> About Us</a>
<ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#grace"><i class="fa fa-check"></i>Grace</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#vivi"><i class="fa fa-check"></i>Vivi</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#yibo"><i class="fa fa-check"></i>Yibo</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#xinyan"><i class="fa fa-check"></i>Xinyan</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#drew"><i class="fa fa-check"></i>Drew</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#as-a-team"><i class="fa fa-check"></i>As a Team</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html"><i class="fa fa-check"></i><b>3</b> Overview of AI Biases</a>
<ul>
<li class="chapter" data-level="3.1" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#types-of-biases"><i class="fa fa-check"></i><b>3.1</b> Types of Biases</a></li>
<li class="chapter" data-level="3.2" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#source-of-biases"><i class="fa fa-check"></i><b>3.2</b> Source of Biases</a></li>
<li class="chapter" data-level="3.3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#examples"><i class="fa fa-check"></i><b>3.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html"><i class="fa fa-check"></i><b>4</b> Overview of AI and LLM</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#artificial-intelligence"><i class="fa fa-check"></i><b>4.1</b> Artificial Intelligence</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#narrow-ai"><i class="fa fa-check"></i><b>4.1.1</b> Narrow AI</a></li>
<li class="chapter" data-level="4.1.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#general-ai"><i class="fa fa-check"></i><b>4.1.2</b> General AI</a></li>
<li class="chapter" data-level="4.1.3" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works"><i class="fa fa-check"></i><b>4.1.3</b> How It Works</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#large-language-models"><i class="fa fa-check"></i><b>4.2</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works-1"><i class="fa fa-check"></i><b>4.2.1</b> How It Works</a></li>
<li class="chapter" data-level="4.2.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-is-trained"><i class="fa fa-check"></i><b>4.2.2</b> How It Is Trained</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>5</b> Case Studies</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-studies.html"><a href="case-studies.html#discrimination-and-unfair-treatment"><i class="fa fa-check"></i><b>5.1</b> Discrimination and Unfair Treatment</a></li>
<li class="chapter" data-level="5.2" data-path="case-studies.html"><a href="case-studies.html#inherent-biases"><i class="fa fa-check"></i><b>5.2</b> Inherent Biases</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html"><i class="fa fa-check"></i><b>6</b> Mitigating Biases in AI Systems</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#companies"><i class="fa fa-check"></i><b>6.1</b> Companies</a></li>
<li class="chapter" data-level="6.2" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#individuals"><i class="fa fa-check"></i><b>6.2</b> Individuals</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html"><i class="fa fa-check"></i><b>7</b> Resources for Further Learning</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#ongoing-research"><i class="fa fa-check"></i><b>7.1</b> Ongoing Research</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bias within the AI Environment</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mitigating-biases-in-ai-systems" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Mitigating Biases in AI Systems<a href="mitigating-biases-in-ai-systems.html#mitigating-biases-in-ai-systems" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="companies" class="section level2 hasAnchor" number="6.1">
  <h2><span class="header-section-number">6.1</span> Companies<a href="mitigating-biases-in-ai-systems.html#companies" class="anchor-section" aria-label="Anchor link to header"></a></h2>

  <!-- Content about IBM -->
  <h3>IBM</h3>
  <p>
    IBM's Fairness 360 toolkit aims to promote a deeper understanding of fairness metrics and mitigation techniques. The toolkit provides a common platform for fairness researchers and industry practitioners to share and benchmark their algorithms. It includes over 71 bias detection metrics, 9 bias mitigation algorithms, and an extensible metric explanation facility.
  </p>
  <p>
    AIF360 uses techniques from 8 published papers on the border algorithm fairness community. It offers various fairness measures, including difference of means, disparate impact, and odds ratio. Some open-source libraries like FairML, FairTest, Aequitas, and Themis-ML provide functionalities for learning fair AI models and auditing predictive models for bias analysis.
  </p>

  <!-- Content about Google -->
  <h3>Google Responsible AI Practices</h3>
  <p>
    Google emphasizes fairness in AI systems and recognizes AI's role in providing new experiences and abilities globally. AI's impact extends beyond recommendations to critical tasks like medical predictions and decision-making. Unfairness in AI systems can have widespread impacts, making fairness crucial. Challenges include biases in real-world data and the difficulty of building universally fair systems due to diverse situations.
  </p>
  <p>
    Google also addresses interpretability, privacy, and safety in AI. Interpretability is vital for understanding and trusting AI systems, while privacy concerns arise when dealing with sensitive data. Google actively develops techniques to safeguard privacy in AI systems. Regarding safety, Google emphasizes proactive safety measures and adapting to emerging attack methods.
  </p>

  <!-- Content about OpenAI -->
  <h3>OpenAI</h3>
  <p>
    OpenAI's behavior in models like ChatGPT is shaped through a two-step process: pre-training and fine-tuning. In the pre-training phase, the model predicts words from Internet text to learn grammar, facts, and reasoning abilities. The fine-tuning phase narrows down system behavior using human reviewers and guidelines. OpenAI aims to align models with human values and improve their behavior.
  </p>
  <p>
    OpenAI addresses biases by involving reviewers and sharing guidelines. They recognize that biases in AI are bugs, not features. OpenAI shares insights, is accountable for sound policies, and continually improves the models. They also emphasize transparency, accountability, and collaboration with the tech community to achieve understandable and controllable fine-tuning.
  </p>

</div>

<div id="individuals" class="section level2 hasAnchor" number="6.2">
  <h2><span class="header-section-number">6.2</span> Individuals<a href="mitigating-biases-in-ai-systems.html#individuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
  
  <!-- START_HIDDEN_CONTENT -->
  <!-- Content about Individuals -->
  <h3>As individuals: Responsible Prompting/building as a Mitigation Technique</h3>
  <p>
    Learn Prompting is a technique that can be used effectively to mitigate biases in AI systems. It involves crafting prompts that encourage AI systems to produce unbiased and fair responses.
  </p>

  <h3>HBR: What Do We Do About the Biases in AI?</h3>
  <p>
    This article discusses the challenges and strategies to address biases in AI systems. It emphasizes the need for awareness and reduction of biases in AI deployment.
  </p>

  <h3>Human Biases and AI</h3>
  <p>
    It's crucial to be aware of biases and their impact on AI systems. Biases in AI deployment can lead to significant risks. There's an urgent need for reducing biases and ensuring fairness.
  </p>
  <!-- ... (rest of the content) ... -->
  <!-- END_HIDDEN_CONTENT -->

</div>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="case-studies.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="resources-for-further-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-blocks.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
