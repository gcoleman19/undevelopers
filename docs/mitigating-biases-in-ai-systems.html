<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Mitigating Biases in AI systems | Bias within the AI Environment</title>
  <meta name="description" content="Chapter 6 Mitigating Biases in AI systems | Bias within the AI Environment" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Mitigating Biases in AI systems | Bias within the AI Environment" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Mitigating Biases in AI systems | Bias within the AI Environment" />
  
  
  

<meta name="author" content="Drew, Grace, Vivi, Xinyan, Yibo" />


<meta name="date" content="2023-08-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="case-studies.html"/>
<link rel="next" href="resources-for-further-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> url: your book url like https://bookdown.org/yihui/bookdown</a></li>
<li class="chapter" data-level="2" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i><b>2</b> About Us</a>
<ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#grace"><i class="fa fa-check"></i>Grace</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#vivi"><i class="fa fa-check"></i>Vivi</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#yibo"><i class="fa fa-check"></i>Yibo</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#xinyan"><i class="fa fa-check"></i>Xinyan</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#drew"><i class="fa fa-check"></i>Drew</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#as-a-team"><i class="fa fa-check"></i>As a Team</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html"><i class="fa fa-check"></i><b>3</b> Overview of AI Biases</a>
<ul>
<li class="chapter" data-level="3.1" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#types-of-biases"><i class="fa fa-check"></i><b>3.1</b> Types of Biases</a></li>
<li class="chapter" data-level="3.2" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#source-of-biases"><i class="fa fa-check"></i><b>3.2</b> Source of Biases</a></li>
<li class="chapter" data-level="3.3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#examples"><i class="fa fa-check"></i><b>3.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html"><i class="fa fa-check"></i><b>4</b> Overview of AI and LLM</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#artificial-intelligence"><i class="fa fa-check"></i><b>4.1</b> Artificial Intelligence</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#narrow-ai"><i class="fa fa-check"></i><b>4.1.1</b> Narrow AI</a></li>
<li class="chapter" data-level="4.1.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#general-ai"><i class="fa fa-check"></i><b>4.1.2</b> General AI</a></li>
<li class="chapter" data-level="4.1.3" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works"><i class="fa fa-check"></i><b>4.1.3</b> How It Works</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#large-language-models"><i class="fa fa-check"></i><b>4.2</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works-1"><i class="fa fa-check"></i><b>4.2.1</b> How It Works</a></li>
<li class="chapter" data-level="4.2.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-is-trained"><i class="fa fa-check"></i><b>4.2.2</b> How It Is Trained</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>5</b> Case Studies</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-studies.html"><a href="case-studies.html#discrimination-and-unfair-treatment"><i class="fa fa-check"></i><b>5.1</b> Discrimination and Unfair Treatment</a></li>
<li class="chapter" data-level="5.2" data-path="case-studies.html"><a href="case-studies.html#inherent-biases"><i class="fa fa-check"></i><b>5.2</b> Inherent Biases</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html"><i class="fa fa-check"></i><b>6</b> Mitigating Biases in AI systems</a></li>
<li class="chapter" data-level="7" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html"><i class="fa fa-check"></i><b>7</b> Resources for Further Learning</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#ongoing-research"><i class="fa fa-check"></i><b>7.1</b> Ongoing Research</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bias within the AI Environment</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mitigating-biases-in-ai-systems" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Mitigating Biases in AI systems<a href="mitigating-biases-in-ai-systems.html#mitigating-biases-in-ai-systems" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>##IBM
The AI Fairness 360 (AIF360) toolkit provided by IBM aims to promote a deeper understanding of fairness metrics and mitigation techniques, to enable an open common platform for fairness researchers and industry practitioners to share and benchmark their algorithms, and to help facilitate the transition of fairness research algorithms to use in an industrial setting.</p>
<p>The AI Fairness 360 (AIF360) toolkit provided by IBM aims to promot a deeper understanding of fairness metrics and mitigation techniques, to enable an open common platform for fairness researchers and industry practitioners to share and benchmark their algorithms, and to help facilitate the transition of fairness research algorithms to use in an industrial setting.</p>
<p>AIF360 uses techniques from 8 published papers on the border algorithm fairness community. It includes over 71 bias detection metrics, 9 bias mitigation algorithms, and a unique extensible metric explanation facility to help consumers of the system to understand the meaning of bias detection results</p>
<p><strong>Definitions Used in AIF360</strong>
1. Fairness measures → provide several fairness metrics including difference of means, disparate impact, and odds ratio</p>
<ol start="2" style="list-style-type: decimal">
<li><p>FairML → Provides an auditing tool for predictive models by quantifying the relative effects of various inputs on a model’s predictions</p></li>
<li><p>FairTest → Checks for associations between predicted labels and protected attributes. Provides user with ability to identify regions of the input space where ana glorithm might incur unusually high errors</p></li>
<li><p>Aequitas → Auditing toolkit for data scientists and policy makers. It has a Python library as well as an associated website where data can be uploaded for bias analysis</p></li>
<li><p>Themis-ML → Repository that provides a few fairness metrics, such as mean difference, and some bias mitigation algorithms, such as relabeling, additive counterfactually fair estimator, and reject option classification.
Paradigm and Architecture</p></li>
</ol>
<p><strong>Paradigm and Architecture</strong>
Fairness pipeline that shows data loading into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from the classifier.</p>
<p><strong>Dataset Classes</strong>
- Training data is used to learn classifiers
- Testing data is used to make predictions and compare metrics
- Besides these standard aspects of the machine learning pipeline, fairness applications also require associating protected attributes with each instance or record in the data</p>
<p><strong>Bias Mitigation</strong>
AIF360 attempts to improve the fairness metrics by modifying the training data, the learning algorithm, or the predictions - also known as pre-processing, in-processing, and post-processing respectively</p>
<p><strong>Maintaining Code Quality</strong>
1. Establishing and maintaining high-quality code is crucial for an evolving open-source system.
2. The AIF360 Github repository is directly integrated with Travis CI, a continuous testing and integration framework, which invoked “pytest” to run unit tests.
- Unit test cases ensure that classes and functions defined in the different libraries are functionally correct and do not break the fairness detection and mitigation pipeline flow.
3. The toolkit is enforced in both front-end and back-end services</p>
<p>##Google Responsible AI Practices
<strong>Fairness</strong>
AI systems influence goes beyond mere recommendations, encompassing critical functions such as decision-making in business. These automated systems possess the potential to enhance fairness and inclusivity on a larger level compared to human-centric judgments. It is vital to recognize that unfairness within AI systems can have far-reaching consequences, inherently showing the importance of fairness.</p>
<p>However, achieving fairness in AI is not without its challenges. Machine learning models are trained on real-world data, which inherently carries biases. Constructing universally fair systems is complicated due to the myriad of diverse situations they encounter. Additionally, fairness lacks a standardized definition and involves a multitude of considerations that must be addressed.</p>
<p>Efforts to ensure fairness in AI necessitate ongoing research and dedication. This involves fostering diversity within the workforce and knowledge base, meticulous scrutiny of training data for biases, training models to rectify these biases, evaluating models for any performance disparities, and subjecting final systems to rigorous fairness testing.</p>
<p>Furthermore, AI has the capacity to identify human biases and barriers, which can contribute to fostering positive societal changes. The pursuit of fairness in AI remains a dual-edged prospect—an opportunity for progress and a complex challenge to conquer. Companies like Google are committed to advancing fairness, offering tools and resources to the broader community to contribute to this ongoing endeavor.</p>
<p><strong>Interpretability</strong>
Automated predictions and decision-making play a pivotal role in enhancing various parts of life. In this context, the concept of interpretability assumes a crucial role, serving as an anchor for questioning, comprehending, and instilling trust in AI systems. By providing interpretability, AI systems contribute to the alignment of domain expertise, societal principles, and the development of models. Interestingly, the challenges surrounding explaining decisions are shared by both AI and human decision-making processes. Unlike humans, AI systems have the capacity to furnish in-depth insights into the rationale behind their predictions, an attribute that sets them apart. However, comprehending intricate AI models, including neural networks, remains a formidable task, even for seasoned experts.</p>
<p>The evaluation of AI systems introduces unique challenges that distinguish them from conventional software. While traditional software follows explicit if-then rules, AI operates through intricate parameter pathways. The tenets of responsible AI design allow for the tracing of values back to training data, facilitating the detection of bugs and anomalies. The comprehension of AI hinges upon its training data, the underlying process, and the resultant model. Collective endeavors within the technology community are actively contributing to the enhancement of understanding, control, and debugging of AI systems.</p>
<p>In this context, Google emerges as a prominent participant, engaging in ongoing research and development endeavors aimed at refining the interpretability of AI.</p>
<p><strong>Privacy</strong>
Machine learning models derive insights from training and input data, raising privacy concerns when handling sensitive information. Achieving a balance between the advantages of using sensitive data and the associated privacy implications is crucial. This encompasses legal, regulatory, social, and individual factors. Implementing safeguards is essential to preserve individual privacy while employing ML models, which necessitates transparent practices and user data control. Techniques can mitigate the likelihood of models exposing underlying data. Google is at the forefront of developing strategies to safeguard privacy within AI systems, contributing to an evolving field of research with substantial growth potential. Their willingness to share insights further enhances the collective understanding of privacy protection in the AI domain.</p>
<p><strong>Safety</strong>
Ensuring AI systems align with intentions and remain resilient against attacks is integral to safety and security. This holds particular importance in safety-critical applications, necessitating robust safety measures prior to deployment. Addressing safety challenges becomes intricate in scenarios characterized by unpredictability and generative AI. This complexity is magnified in intricate problems and generative AI. Striking a balance between proactive safety precautions and accommodating creative adaptability presents a formidable task. The evolution of AI introduces novel attack vectors, prompting an ongoing pursuit of effective solutions. Drawing from accumulated insights, it’s advisable to prioritize safety during pre-deployment planning for critical applications, anticipate diverse scenarios—particularly in generative AI settings—maintain equilibrium between safety protocols and creative latitude, and remain flexible in countering emerging attack strategies through adaptive countermeasures.</p>
<p><strong>Recommended Practices</strong>
Research in machine learning safety encompasses various threats including data poisoning, data recovery, model theft, and adversarial examples. Google plays an active role in investigating these domains, some of which intersect with AI and privacy concerns. A specific emphasis lies on adversarial learning, where one network crafts deceptive instances and another discerns fraudulent behavior. Though ongoing, the establishment of dependable defenses against adversarial examples remains in progress. Developers are advised to assess potential attacks and their ramifications, steering clear of vulnerable system designs. Adversarial testing systematically scrutinizes ML models using malicious input to bolster comprehension and mitigation efforts. This process aids in recognizing patterns of failure and guides enhancements through fine-tuning and protective measures.</p>
<p>##OpenAI
<strong>How ChatGPT’s Behavior is Shaped</strong>
ChatGPT’s model is a massive neural network, unlike ordinary software. The behavior of the model is learned from diverse data, not explicitly programmed. The process is similar to training a dog, as oposed to traditional programming. There are two phases of training the model, the first of which is the “pre-training” phase. This phase consists of teaching the model to predict the next words from Internet text. Second is the “fine-tuning” phase which fine-tunes the system’s behavior. The model is also constantly improving its alighment with human values. Here is a more in-depth explanation of these training phases and processes:</p>
<ol style="list-style-type: decimal">
<li>Pre-Training: Predicting the next words from the Internet dataset</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Learn grammar, facts, reasoning abilities</li>
<li>Absorb biases from the data</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li>Fine-Tuning: Narrow dataset with human reviewers</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Reviewers follow guidelines</li>
<li>Not all possible inputs covered</li>
<li>Categories in guidelines for model responses</li>
<li>Models generalize from reviewer feedback</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Reviewers and OpenAI policies in system development:
OpenAI maintains a close relationship with its reviewers and adheres to its established policies. This includes providing clear instructions on the types of outputs expected from the model. OpenAI offers high-level guidance, particularly with regards to handling controversial topics. Collaboration with reviewers is a continuous effort, involving weekly meetings to exchange feedback and seek clarifications. The development process follows an iterative approach, enabling consistent refinement of the model for improvement over time.</li>
</ol>
<p><strong>Role of Reviewers and OpenAI’s Policies in System Development</strong></p>
<ol style="list-style-type: decimal">
<li>Reviewer Guidance:</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Specific output cases (e.g., no illegal content)</li>
<li>High-level guidance (e.g., avoid controversial stances)</li>
<li>Ongoing collaboration, not one-time</li>
<li>Learning from reviewer expertise</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li>Feedback Loop and Fine-Tuning</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Strong feedback loop with reviewers</li>
<li>Weekly meetings to address questions</li>
<li>Clarify guidance as needed</li>
<li>Iterative process for continuous improvement</li>
</ol>
<p><strong>Addressing Bias</strong>
1. Addressing Biases and Transparency:
OpenAI acknoledges the concerns about AI biases, and is commited to addressing them. OpenAI sees bias as bugs, as opposed to features of the model. In order to tackle the issue, they strive to be transparent in their intentions and progress in building the model. In addition, there is no favoring of any political groups by reviewers.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Sharing Insights and Accountability:
OpenAI shares their guidelines for user insight, and to stay accountable for uphering sound policies.</p></li>
<li><p>Continuous Improvement:
In order to improve the model, OpenAI is constantly enhancing their guidelines for clarity, making clearer instructions for reviewers, addressing bias, controversial figures, and themes. Additionaly, they are transparent in the demographic information of their reviewers, continuously research understandable and controllable fine-tuning, and build on external advances like rule-based rewards and Constitutional AI</p></li>
</ol>
<p><strong>The Building Blocks of Future Systems</strong>
The overarching mission centers on ensuring widespread access, benefits, and impact of AI and AGI. To achieve this, three fundamental building blocks are in place. The first focuses on enhancing the default behavior of AI, aiming for usefulness and respectfulness right from the start. This involves comprehensive research and engineering to mitigate biases, rectify over-refusals and under-refusals, and leverage user feedback to drive improvements. The second block revolves around defining AI’s values within appropriate boundaries, facilitating customizable AI systems that align with societal limits. This entails refining ChatGPT for user-driven behavior customization while striking a balance between customization potential and guarding against misuse risks. The goal is to set well-defined boundaries for AI behavior to prevent the concentration of power. The third block emphasizes public involvement in shaping defaults and boundaries to prevent undue centralization of influence. Through collective decision-making, public participation is sought in determining system defaults and behavior boundaries, with external input garnered through red teaming. This extends to soliciting public insights on AI in education and piloting initiatives for public contributions on behavior, disclosure, and policies. Furthermore, the exploration of third-party safety and policy audits underscores a commitment to transparency and accountability.</p>
<p>##What Do We Do About the Biases in AI?
<strong>Human Biases and AI</strong></p>
<p><strong>Historical Bias and Complex Algorithms</strong>
Throughout history, various examples illustrate the insidious nature of bias in AI. Instances where facial recognition systems struggled to accurately identify individuals with darker skin tones serve as a stark reminder of algorithmic bias. While AI has the potential to mitigate biases by automating decision-making, the complexity of algorithms can also amplify existing biases when trained on skewed datasets. Understanding this balance is pivotal in harnessing AI’s potential for societal benefit.</p>
<p><strong>Addressing Bias in AI</strong></p>
<p><strong>Imperatives for Action</strong></p>
<p><strong>CEO and Leadership Steps</strong></p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="case-studies.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="resources-for-further-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-blocks.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
