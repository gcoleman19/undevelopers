<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Case Studies | Bias within the AI Environment</title>
  <meta name="description" content="Chapter 5 Case Studies | Bias within the AI Environment" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Case Studies | Bias within the AI Environment" />
  <meta property="og:type" content="book" />


  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Case Studies | Bias within the AI Environment" />




<meta name="author" content="Drew, Grace, Vivi, Xinyan, Yibo" />


<meta name="date" content="2023-08-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="overview-of-ai-and-llm.html"/>
<link rel="next" href="mitigating-biases-in-ai-systems.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body>
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">
<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i><b>2</b> About Us</a>
<ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#grace"><i class="fa fa-check"></i>Grace</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#vivi"><i class="fa fa-check"></i>Vivi</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#yibo"><i class="fa fa-check"></i>Yibo</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#xinyan"><i class="fa fa-check"></i>Xinyan</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#drew"><i class="fa fa-check"></i>Drew</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#as-a-team"><i class="fa fa-check"></i>As a Team</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html"><i class="fa fa-check"></i><b>3</b> Overview of AI Biases</a>
<ul>
<li class="chapter" data-level="3.1" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#types-of-biases"><i class="fa fa-check"></i><b>3.1</b> Types of Biases</a></li>
<li class="chapter" data-level="3.2" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#source-of-biases"><i class="fa fa-check"></i><b>3.2</b> Source of Biases</a></li>
<li class="chapter" data-level="3.3" data-path="overview-of-ai-biases.html"><a href="overview-of-ai-biases.html#examples"><i class="fa fa-check"></i><b>3.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html"><i class="fa fa-check"></i><b>4</b> Overview of AI and LLM</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#artificial-intelligence"><i class="fa fa-check"></i><b>4.1</b> Artificial Intelligence</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#narrow-ai"><i class="fa fa-check"></i><b>4.1.1</b> Narrow AI</a></li>
<li class="chapter" data-level="4.1.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#general-ai"><i class="fa fa-check"></i><b>4.1.2</b> General AI</a></li>
<li class="chapter" data-level="4.1.3" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works"><i class="fa fa-check"></i><b>4.1.3</b> How It Works</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#large-language-models"><i class="fa fa-check"></i><b>4.2</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-works-1"><i class="fa fa-check"></i><b>4.2.1</b> How It Works</a></li>
<li class="chapter" data-level="4.2.2" data-path="overview-of-ai-and-llm.html"><a href="overview-of-ai-and-llm.html#how-it-is-trained"><i class="fa fa-check"></i><b>4.2.2</b> How It Is Trained</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>5</b> Case Studies</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-studies.html"><a href="case-studies.html#discrimination-and-unfair-treatment"><i class="fa fa-check"></i><b>5.1</b> Discrimination and Unfair Treatment</a></li>
<li class="chapter" data-level="5.2" data-path="case-studies.html"><a href="case-studies.html#inherent-biases"><i class="fa fa-check"></i><b>5.2</b> Inherent Biases</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html"><i class="fa fa-check"></i><b>6</b> Mitigating Biases in AI Systems</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#companies"><i class="fa fa-check"></i><b>6.1</b> Companies</a></li>
<li class="chapter" data-level="6.2" data-path="mitigating-biases-in-ai-systems.html"><a href="mitigating-biases-in-ai-systems.html#individuals"><i class="fa fa-check"></i><b>6.2</b> Individuals</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html"><i class="fa fa-check"></i><b>7</b> Resources for Further Learning</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resources-for-further-learning.html"><a href="resources-for-further-learning.html#ongoing-research"><i class="fa fa-check"></i><b>7.1</b> Ongoing Research</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>
      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bias within the AI Environment</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="case-studies" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> <strong>Case Studies</strong><a href="case-studies.html#" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In our modern world where artificial intelligence (AI) is entwined with daily life, the discovery of biases within algorithms has unveiled a troubling reflection of societal prejudices. These biases, both unintentional and intentional, reverberate through sectors like hiring, lending, and criminal justice, influencing lives in profound ways. However, alongside these unintentional biases, there exists a complex landscape of intentional biases applied with the goal of promoting fairness, addressing historical injustices, or achieving specific societal objectives.</p>
<p>Two groundbreaking studies cast a humanizing light on this technological dilemma. <a href="https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">Gender Shades by Joy Buolamwini and Timnit Gebru</a> explored the troubling racial and gender disparities in facial recognition, revealing how these technologies can unintentionally mirror and magnify existing inequalities. Meanwhile, <a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf">Man is to Computer Programmer as Woman is to Homemaker?</a> Debiasing Word Embeddings by Bolukbasi et al. delved into gender biases within language algorithms, unearthing the subtle ways in which technology can perpetuate stereotypes. Both studies serve as poignant reminders that behind every algorithm, there are human lives and societal norms that deserve careful consideration and compassion.</p>
<div id="biases-in-hiring-lending-criminal-justice" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> <strong>Biases in hiring, lending, criminal justice:</strong><a href="case-studies.html#biases-in-hiring-lending-criminal-justice" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong><a href="https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">Gender Shades” by Joy Buolamwini and Timnit Gebru:</a> </strong></p>
<p>Buolamwini and Gebru evaluated commercial facial recognition systems by testing them on a more diverse dataset that included a wider representation of gender, skin color, and ethnic backgrounds. They discovered significant disparities in accuracy across different demographic groups, with the algorithms performing poorly on darker-skinned and female faces.</p>
<p><img src="https://ars.electronica.art/outofthebox/files/2019/08/GenderShades_gs04.jpg" /></p>
<p><strong><em>Key Takeaway:</em></strong></p>
<p><strong>Exposing Disparities:</strong> Their method has shed light on real-world implications of bias within facial recognition technologies, bringing attention to an issue that has substantial societal impact.</p>
<p><strong>Discrimination:</strong> These biases can lead to systematic discrimination, where certain groups may be unfairly targeted or misclassified by automated systems used in law enforcement, hiring, or other critical domains.</p>
<p><strong>Loss of Trust:</strong> Awareness of these biases can erode trust in AI systems and technology more broadly, especially among those most affected by the inaccuracies.</p>
<p><img src="https://globalnews.ca/wp-content/uploads/2018/02/gs03.png" />
The work by Buolamwini and Gebru is pioneering and has had a profound impact on how the industry approaches bias in AI. Their methods are robust and have led to broader conversations about ethical AI. While technological solutions are necessary, a systemic approach involving diverse stakeholders is essential to truly eliminate biases.</p>
<p><strong><a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings” by Bolukbasi et al.:</a></strong></p>
<p><img src="https://2.bp.blogspot.com/-RUpKuflR6qQ/XLwEXB16ozI/AAAAAAAAr4Q/Jo3_0oqOZfYE6rygFvl8CA0MOo5lkS43QCLcBGAs/s1600/Debias1.png" /></p>
<p>The article by Tolga Bolukbasi and others describes a process to reduce gender bias by identifying and modifying the geometric relationships within word embeddings. The authors first identify a gender subspace by looking at pairs of words that reflect gender, such as (“he”, “she”), (“man”, “woman”), etc. They then project word vectors onto this subspace to measure their gender bias and adjust them to reduce this bias, while preserving other semantic relationships.</p>
<p><strong><em>Key Takeaway:</em></strong></p>
<p><strong>Effectiveness:</strong> The methods present a clever way to reduce gender bias in word embeddings, and they have been influential in guiding further research in the field.</p>
<p><strong>Limitations:</strong> While the method addresses gender bias for certain word pairs, it may not completely remove all forms of gender bias, especially more subtle or complex biases.</p>
<p><strong>Ethical Considerations:</strong>Deciding what constitutes a bias and how to address it can be controversial. This method mainly addresses one specific type of bias, and the decision-making process might need to involve broader social input.</p>
<p>The methodology in this paper is an essential step in addressing biases in AI, but it also highlights the complexity of the problem. A multidisciplinary approach, involving technological innovation, human expertise, and societal input, might be the best path forward to build AI systems that are both powerful and aligned with human values.</p>
</div>
<div id="inherent-biases---the-complex-trade-offs-in-designing-fair-machine-learning-models." class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> <strong>Inherent Biases - the complex trade-offs in designing fair machine learning models.</strong><a href="case-studies.html#inherent-biases---the-complex-trade-offs-in-designing-fair-machine-learning-models." class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Intentional biases in machine learning models are indeed sometimes applied with the goal of promoting fairness, addressing historical biases, or achieving specific societal objectives. This is often referred to as “fairness through awareness” or “algorithmic affirmative action.” Below are some examples of intentional biases and related case studies:</em></p>
<p><strong>Equalized Odds in Hiring Practices:</strong> Some hiring algorithms might intentionally favor candidates from underrepresented groups to ensure that the selection process provides equal opportunity to all applicants <a href="https://www.researchgate.net/publication/339105052_Algorithmic_Fairness_from_a_Non%20-ideal_Perspective">(Fazelpour &amp; Lipton, 2020)</a>.</p>
<p><strong>Bias Correction in Credit Scoring:</strong> Intentional adjustment in removing sensitive attributes, such as gender identifiers may aim to provide fair access to credit opportunities by counterbalancing biases against specific demographic groups <a href="https://www.frontiersin.org/articles/10.3389/fdata.2022.1049565/full">(Liang et al., 2023)</a></p>
<p><img src="https://www.frontiersin.org/files/MyHome%20Article%20Library/1049565/1049565_Thumb_400.jpg" /></p>
<p><strong>Intentional Gender Bias in Gender Classification Models:</strong> Many researchers make intentional effort to compile a dataset of facial images representing different genders, ethnicities, and other demographic factors, ensuring a diverse sample that includes non-binary and transgender individuals. Thus, they further encourage the industry to evolve in a direction that recognizes the complexity and diversity of human gender identity <a href="https://doi.org/10.1145/3359246">(Scheuerman et al., 2019)</a></p>
<p><img src="https://media.cnn.com/api/v1/images/stellar/prod/191118130655-ai-gender-identification-morgan.jpg?q=w_3000,h_1688,x_0,y_0,c_fill" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="overview-of-ai-and-llm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mitigating-biases-in-ai-systems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/04-citations.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
