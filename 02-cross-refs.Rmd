# Overview of AI Biases 

What really is AI bias? In short, it is when specific components within a dataset are allowed to be overrepresented or weighted too highly. This, in turn, forces other components to become underrepresented and underweighted. Consequently, accuracy can get affected, results can be misshapen, which allows prejudices to occur. At this point in time, most companies and organizations are beginning to adopt artificial intelligence as part of their overall business intelligence strategy. As far as the how and the why, it’s difficult to parse an agreed upon set of bias types (if the magnitude of the set even closely resembles that of the set of cognitive biases, one can expect over 100). This is mostly because different people use different terms for the same thing. On top of that, bias can find its way into every part of the AI process, namely from the data itself, from the algorithm, or from the people meant to interpret the result. That being said, here are a collection of the most prevalent types of bias:

## Systemic Bias {-}


Systemic bias looks as far outward as possible and examines the ways in which entire social structures and groups are overlooked in favor of others. As the name indicates, this kind of discrimination comes as a result of well established systematic processes that dominate our lives. Most importantly, these processes are often designed in such a way that makes them invisible to most people. Machine learning algorithms that produce AI models are inherently confined to build and interpret from the data that they are given. They primarily function on pattern recognition, which is unfortunately the main heuristic that stereotyping utilizes. Thus when feeding them data that is representative of the systems that we all belong to, they inherit the same invisible discriminations. 
The data that these algorithms operate on is called training data. The reality is that this data is not solely restricted to what a single set of real world measurements can offer us. There are data experts that collect and synthesize information from many different sources into a unified set that can add new categories of measurement or target specific perspectives that might have gone unnoticed. Not to mention the possibility of creating entirely synthetic data from artificial generators, which similarly allows engineers to filter out the unwanted behavior that might come from model interpretation of the data.
A good example of this systematic behavior within AI came out several years ago, when a group of researchers at Carnegie Mellon and the International Computer Science Institute investigated how Google’s Ad Settings chose to target specific ads towards certain people. The results were unfavorable to say the least. Instances of discrimination occurred across the board from Google’s algorithm. A notable case was when the researchers changed the gender on the Ad Settings to female. What they noticed was that ads related to higher paying jobs were reduced in the female setting as opposed to the male. In this case, the model was perpetuating the discriminatory behavior of standard advertisers when they unknowingly reinforce gender roles.

## Automation Bias {-}

On the spectrum of data error versus human error, automation bias stands firmly in the latter category. In simple terms, it is when humans overly rely on automation. Given AI’s totemic position within the vanguard of automated processes, one can clearly see how this form of bias can be abused. Of all of the discourse surrounding AI today, conversations about the future of reliance on AI is perhaps the most zeitgeist piercing. The real cause for concern around this form of bias is the expectation that AI produces infallible results. If the training data on the model is in any way meant to be representative of the real world, then it suffers the same tendencies for inaccuracy that we do. If that data happens to be the Internet itself, then it suffers even greater chances to spread misinformation. We are often told to not make decisions off of a single source. Paradoxically, a result from a single AI model — whose goal it is to scour an innumerable amount of sources — unifies itself into a single source.
In Hugo Mercier and Dan Sperber’s “Why do humans reason? Arguments for an argumentative theory”, they suggest that the evolutionary skill of reasoning served an persuasive function rather than one built for the purpose of knowledge acquisition. Humans inherently (and often unconsciously) start from a conclusion first and search for the appropriate evidence to support it. The implication here is that, on our own, we are actually pretty bad at coming to the most correct conclusion. However, together within an argumentative setting of other people, we are more effectively able to pick out the right components of each argument into a single realized conclusion. The reason to bring this up is that the inherent processes of machine learning does not create its conclusions from argumentation because who would it be arguing against? Even the more advanced machine reasoning, which attempts to replicate human “common sense” through logical processes, is totally at odds with this theory. This misalignment can help to explain why GPS can often lead you down the wrong path in an attempt to find the quickest path.
ChatGPT presents the most visible example of automation bias today. Educators at every level have been struggling to get kids to not use the AI to do work for them whether that be assignments or assessments. Kids will receive answers that, regardless of validity, provide none of the reasoning process that got it there. This inherent problem of not knowing or understanding how a machine learning algorithm got to a certain conclusion is called black-box and it is only increasingly becoming a problem as more and more of the general public adopts AI usage.

## Overfitting and Underfitting {-}

Imagine you are trying to research a certain topic with the goal in mind that you can be trusted to give reliable information regarding that topic. In order to do so, you try to find as many sources as possible. The idea is that you would reduce the amount of key missing information. However, now you have so much data that there’s noise and contradictions. The only way to make a conclusion that fits all of the data is an overly convoluted one. This is one of the fundamental issues happening with machine learning models right now. 
Underfitting is in the former instance when a model isn’t able to capture the trend in testing data despite performing well in the training data. Typically this occurs from insufficient amounts of training data which results in the model producing simplified rules for a more complicated problem. This is where AI can often make biased decisions. Using overly simple patterns, it can generalize these to entire categories of data. The upside is that its variance in accuracy depending on the testing data remains fairly low. This is not to say it’s somehow accurate, just that it’s equally inaccurate across many different testing environments. 
Conversely, overfitting is when there is more than sufficient data but the noise that comes from it allows the model to produce far too complicated results that forces the data to fit. Applying different testing environments creates high variance in inaccuracy even though the model is trying to consider every possible manner of bias. This is the rub, high bias but for low variance, or low bias at the cost of high variance.

![This model provides a great visualization for the pros and cons of underfitting and overfitting and how typically it is beneficial to find a balance between the two.](C:\Users\DrewG\Downloads\undevelopers\overfitting vs underfitting.png)

One of the most often used ways to detect overfitting is called k-fold cross-validation. The idea is to randomly sort the data into k subsections, saving one section to be the testing set and the others will become the training data. Iterate this k times, taking the cumulative sum of the errors to see how well the model performs.

![This is an example of how k-fold cross validation works when k is 5.](C:\Users\DrewG\Downloads\undevelopers\k-fold cross validation.png)
As it must be apparent at this point, neither underfitting nor overfitting is strictly better than the other. More importantly, in tandem they reveal an unfortunate reality of any AI model: it is impossible to create a system with zero bias and zero variance in accuracy. The best we can do is find the optimized minimum of the two.

![](C:\Users\DrewG\Downloads\undevelopers\variance to bias tradeoff.png)

## Reporting bias {-}

Moving outside of machine error, we arrive at the problem of how we choose what data to train an algorithm with. Reporting bias is most often seen when an algorithm is measuring event frequency. The bias happens when the training data does not accurately reflect the frequency of said event in the real world. 
A simplified example might be a sentiment analysis model for determining whether a book review is positive or negative. The issue is that most websites that have such reviews (think Goodreads) have an unusually high amount of extreme reviews. This is because most people will only write a review if they really loved or they really hated this. This means that the algorithm will be unequipped to interpret any subtleties. Thus, when trying to synthesize an appropriate dataset for training, we run into a problem: all the most available sources are biased against the intended result.


## Overgeneralization bias {-}

When we look at the output that an AI model generates for us we often take it as gospel. There’s an expectation that what we can see from one dataset can be extrapolated to other datasets accessing the same information but this isn’t usually the case. Machine learning models fundamentally work from probabilities, and imperfect probability is the enemy of repeatable certainty.

## Group Attribution bias {-}

When data engineers are in the process of creating an algorithm, they might favor certain factors that they can attribute to themselves over factors that exist to outside groups. This creates the dichotomy of the in-group versus out-group. Where this type of bias has been prevalent is in the use of AI for hiring and recruitment practices. For example, say the engineers added some extra weight to the particular undergraduate institutions they attended. There is no reason to believe that individuals who attended those institutions exhibit the required skills for the position any more so than those who went to other universities.