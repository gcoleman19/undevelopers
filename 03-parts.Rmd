# Overview of AI and LLM

## Artificial Intelligence

Artificial Intelligence (AI) is a captivating field in computer science that aims to create systems capable of mimicking human intelligent behaviors. The history of this concept dates back to the mid-20th century when computer scientist Alan Turing introduced the famous Turing Test, defining AI as systems that exhibit actions "like those of a human being." Despite controversies surrounding the Turing Test at the time, it laid the foundation for later AI research, defining AI as systems that can demonstrate human-like intelligence.

Stuart Russell and Peter Norvig, authors of "Artificial Intelligence: A Modern Approach," identified four potential goals of AI, which influence its development direction. These goals differentiate AI systems based on their rationality and thinking versus acting:

1.  Systems that think like humans.
2.  Systems that act like humans.
3.  Systems that think rationally.
4.  Systems that act rationally.

These goals emphasize the positive impact of AI on human society and values, not just technological advancement.

At its simplest level, AI combines computer science with powerful data processing capabilities to address various problems. This field encompasses subfields like machine learning and deep learning, closely related to AI. Machine learning and deep learning involve training algorithms to recognize patterns and extract information from data. These methods have made significant progress in fields like image recognition, natural language processing, and recommendation systems. These algorithms build expert systems that predict, classify, and make decisions based on input data.

### Narrow AI

Narrow AI, also known as Weak AI or Applied AI, refers to AI systems that focus on and can only solve specific domain problems. Most AI algorithms and applications we see today fall under the category of Narrow AI. Narrow AI excels in performing individual tasks. While these machines may appear intelligent, their capabilities are limited compared to even the most basic human intelligence.

In the realm of Narrow AI technology, research experience accumulated by humans and practices in engineering management and safety regulations often apply. Whether it's a self-driving car or a crane operator system, both require rigorous quality control processes and safety oversight strategies. Errors in autonomous driving programs could lead to accidents, just as mistakes in crane design could cause the crane to tip over, resulting in casualties. In essence, Narrow AI is merely a technological tool, and if it poses risks, they aren't fundamentally different from risks associated with widely-used technologies.

Examples of Narrow AI include chatgpt, virtual assistants like Siri and Alexa, self-driving cars, search engines like Google, conversational robots, spam filters, and Netflix recommendations.

### General AI

General AI, also known as strong AI or Artificial General Intelligence (AGI), is a theoretical form of AI in which machines possess intelligence comparable to human beings. AGI is a concept wherein machines could have intelligence equivalent to human beings, encompassing self-awareness, problem-solving, learning, and future planning abilities. General AI can perform a variety of tasks across different domains and learn to solve new problems autonomously.

While Narrow AI focuses on performing specific tasks, such as answering questions based on user input or playing chess, AGI can perform a variety of functions and eventually learn to solve new problems on its own. Narrow AI relies on human intervention to define the parameters of its learning algorithms and provide relevant training data to ensure accuracy. Although human input accelerates the growth of AGI, it's not necessary, and over time, it could develop consciousness similar to human beings.

Strong AI possesses the following capabilities: Reasoning, strategizing, problem-solving, and decision-making in the presence of uncertainty. Knowledge representation, including common-sense knowledge. Planning. Learning. Communication through natural language. Integration of the above abilities to achieve set goals.

If computer programs could evolve to be smarter than the smartest and most gifted humans in the world, the resulting AI system could be referred to as Artificial Superintelligence (ASI). ASI is a theoretical concept representing an AI system with capabilities and potential far beyond human intelligence. Unlike existing Narrow AI and General AI (AGI), ASI is an immensely powerful and advanced form of intelligence.

### How It Works

AI programming focuses on cognitive skills including learning (acquiring data and converting it into actionable information, i.e., algorithms), reasoning, self-correction, and creativity. The workflow of AI involves:

**Input:** Engineers gather the data needed for AI to operate normally, such as text, images, and speech. Ensuring that algorithms can read this input data is crucial. Contextualizing the data and defining the desired outcomes are important.

**Processing:** AI interprets the pre-programmed data and uses the
behaviors it has learned to recognize the same or similar behavior
patterns in real-time data.

**Data Outcomes:** Determining whether the data and its given
predictions are failures or successes.

**Adjustment:** If the dataset fails, AI can learn from the mistakes and repeat the process in a different way. The rules of the algorithm may need adjustments or changes to suit the dataset. This stage may see changes in outcomes to reflect more ideal or suitable results.

**Assessment:** Once AI completes the assigned task, the final step is assessment. This stage allows the technology to analyze the data and make inferences and predictions. It also provides useful feedback before running the algorithm again.

AI systems work by ingesting large amounts of labeled training data and combining rapid iterative processing with intelligent algorithms. They analyze the data's correlations and patterns, allowing software to learn patterns or features from data automatically. This way, a chatbot fed with examples of text conversations can learn to engage in convincing conversations, or an image recognition tool can learn to identify and describe objects in images by observing millions of examples. Emerging generative AI technologies create realistic text, images, music, and other media.

The core principle of AI is leveraging data-driven methods, deep
learning, and machine learning to process information, extract relevant patterns, and generate responses or outputs that resemble human intelligence.

## Large Language Models

### How It Works

![](lm.png)

Large Language Models (LLMs) utilize deep learning techniques, particularly neural networks, to comprehend and generate text similar to human natural language. These models are trained on extensive text data to capture language patterns, syntax, semantics, and context. The working mechanism of large language models involves but is not limited to the following steps:

**Tokenization:** Input text is divided into smaller units called
tokens, which can be words, subwords, or even characters, depending on the model's architecture and training data.

**Embedding:** Each token is mapped to a high-dimensional vector
representation known as embedding. These embeddings capture semantic relationships between words, aiding the model in understanding context.

**Architecture:** LLMs typically employ Transformer-based architectures. Transformers enable parallel processing of input data while capturing long-range dependencies in the text.

**Training:** LLMs undergo unsupervised learning during training on
massive text corpora. They predict the next word in a sequence given the previous word context. By minimizing the difference between the predicted word and the actual word in the training data, the model learns to generate coherent and contextually appropriate text.

Large language models work by learning statistical patterns and
structures from vast amounts of text data. They capture relationships between words, understand context, and generate text that simulates human language. However, LLMs generate text based on learned patterns from training, which might not truly understand language like humans.

### How It Is Trained

Training large language models requires providing a substantial amount of textual data to enable the model to learn the structure, syntax, and semantics of human language. This process is typically accomplished through unsupervised learning, with one commonly used technique being self-supervised learning. In self-supervised learning, the model generates its own labels for input data by predicting the next word or token in a sequence, thereby establishing associations between words.

The training process can be divided into two main steps: pretraining and fine-tuning.

During the pretraining phase, the model learns on a vast and diverse dataset. This dataset usually encompasses billions of vocabulary items from various sources such as websites, books, and articles. In the pretraining phase, the model learns general language patterns and representations from a large amount of data. This allows it to understand relationships between different words, context, and common grammar structures. After the pretraining phase, the fine-tuning phase occurs, which involves training the model on a more specific and smaller dataset related to the target task or domain. Through training on these specific data, the model can better adapt to the requirements of the particular task and further enhance its understanding capabilities.

The datasets used for pretraining and fine-tuning are different. The
pretraining dataset is usually broad and generalized, while the fine-tuning dataset is more focused on the specific task the model is meant to solve. This two-step training strategy allows the model to acquire general language capabilities from extensive general data and then enhance its performance through fine-tuning on task-specific data.
